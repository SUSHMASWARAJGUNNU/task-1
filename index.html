<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Xml Project</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <center>
      <section class="container"> <section class="container1">
        <h1>Chapter V</h1>
        <h1>Parallel and Distributed<br>
            Data Mining through<br>
            Parallel Skeletons and<br>
            Distributed Objects</h1>
        <h6>Massimo Coppola<br>University of Pisa, Italy</h6>
            <h6>Marco Vanneschi<br>University of Pisa, Italy</h6>
            <h2>ABSTRACT</h2>
                         <p>We consider the application of parallel programming environments to develop portable
                            and efficient high performance data mining (DM) tools. We first assess the need of
                            parallel and distributed DM applications, by pointing out the problems of scalability
                            of some mining techniques and the need to mine large, eventually geographically
                            distributed databases. We discuss the main issues of exploiting parallel and distributed
                            computation for DM algorithms. A high-level programming language enhances the
                            software engineering aspects of parallel DM, and it simplifies the problems of integration
                            with existing sequential and parallel data management systems, thus leading to
                            programming-efficient and high-performance implementations of applications. We
                            describe a programming environment we have implemented that is based on the
                            parallel skeleton model, and we examine the addition of object-like interfaces toward
                            external libraries and system software layers. This kind of abstractions will be included
                            in the forthcoming programming environment ASSIST. In the main part of the chapter,
                            as a proof-of-concept we describe three well-known DM algorithms, Apriori, C4.5, and
                            DBSCAN. For each problem, we explain the sequential algorithm and a structured
                            parallel version, which is discussed and compared to parallel solutions found in the</p><br>
       </section> <br>
        
         <section class="container2"> <p>literature. We also discuss the potential gain in performance and expressiveness from
                            the addition of external objects on the basis of the experiments we performed so far. We
                            evaluate the approach with respect to performance results, design, and implementation
                            considerations.</p>
        <h2>INTRODUCTION</h2>
                    <p>The field of knowledge discovery in databases, or Data Mining (DM), has evolved
                        in the recent past to address the problem of automatic analysis and interpretation of larger
                        and larger amounts of data. Different methods from fields such as machine learning,
                        statistics, and databases, just to name a few, have been applied to extract knowledge from
                        databases of unprecedented size, resulting in severe performance and scalability issues.
                        As a consequence, a whole new branch of research is developing that aims to exploit
                        parallel and distributed computation in the computationally hard part of the mining task.
                        The parallelization of DM algorithms, in order to find patterns in terabyte datasets in realtime, has to confront many combined problems and constraints, e.g., the irregular,
                        speculative nature of most DM algorithms, data physical management, and issues typical
                        of parallel and distributed programming, like load balancing and algorithm decomposition. Fast DM of large or distributed data sets is needed for practical applications, so the
                        quest is not simply for parallel algorithms of theoretical interest. To efficiently support
                        the whole knowledge discovery process, we need high-performance applications that are
                        easy to develop, easy to migrate to different architectures, and easy to integrate with
                        other software. We foster the use of high-level parallel programming environments to
                        develop portable and efficient high-performance DM tools. An essential aspect of our
                        work is the use of structured parallelism, which requires the definition of the parallel
                        aspects of programs by means of a fixed, formal definition language. High-level parallel
                        languages of this kind shelter the application programmer from the low-level details of
                        parallelism exploitation, in the same way that structured sequential programming separates the complexity of hardware and firmware programming models from sequential
                        algorithm design. Structured parallel languages are a tool to simplify and streamline the
                        design and implementation of parallel programs.</p>
                    <p>
                            A common issue in DM and in high-performance computing is the need to efficiently
                            deal with a huge amount of data in complex memory hierarchies. Managing huge input
                            data and intermediate results in the former case, and avoiding excessive amounts of
                            communications in the latter case, highly complicate the algorithms and their implementation. Even if current research trends aim at pushing more and more of the mining task
                            into the database management support (DBMS), and at developing massively parallel
                            DBMS support, the problem of scaling up such support beyond the limits of sharedmemory multiprocessors is yet to be solved. In our view, high-level programming
                            environments can also provide a higher degree of encapsulation for complex data
                            management routines, which at run-time exploit the best in-core and out-of-core techniques, or interface to existing, specialized software support for the task. The enhanced
                            interoperability with existing software is definitely a great advantage in developing highperformance, integrated DM applications. We will sustain our perspective by showing
                            how to apply a structured parallel programming methodology based on skeletons to DM</p>
        </section ><br>

         <section class="container3" > <p>problems, also reporting test results about commonly used DM techniques, namely
                            association rules, decision tree induction, and spatial clustering.</p>
                <p> In the first part of the chapter, we provide a background about parallel DM and
                            parallel programming. The following section analyzes the general problems of DM
                            algorithms, discussing why parallel and distributed computation are needed, and what
                            are the advantages and issues they bring us. The integration issues with other applications and with DBMS supports are especially important. Two sections recall the
                            structured parallel programming approach and its application within the skeleton
                            language of the SkIE environment. One more section is devoted to the problems coming
                            from explicit management of the memory hierarchy. The first part of the chapter ends with
                            the proposal of a common interface from parallel applications to external services, based
                            on the object abstraction. We discuss the advantages that this new feature brings in
                            terms of enhanced modularity of code, and the ability to interface with different data
                            management software layers. The experiences made with the development of SkIE and
                            of this kind of language extensions will be used in the design of the second-generation
                            structured parallel language called ASSIST</p> 
                            <p>
                                In the second part of the chapter, we show three well-known DM algorithms and
                                their skeleton implementation. We discuss association rule mining, classification ,and
                                clustering in three sections, according to a common presentation outline. We define each
                                one of the DM problems, and we present a sequential algorithm that solves it. They are
                                respectively Apriori, C4.5, and DBSCAN. We explain how the algorithm can be made
                                parallel and its expected performance, discussing related approaches in the literature. We
                                present skeleton structures that implement the parallel algorithm and describe its
                                characteristics. The structured parallel C4.5 also uses a first prototype of external object
                                library. A final section reports test results performed with real applications implemented
                                with SkIE over a range of parallel architectures. To assess the performance and portability
                                aspects of the methodology, we discuss the development costs of the prototypes. We
                                conclude by pointing out future developments and open research issues.</p><br>
                                <h2>PARALLEL AND DISTRIBUTED DATA MINING</h2><br>
                                <p>
                                    The need for high-performance DM techniques grows as the size of electronic
                                    archives becomes larger. Some databases are also naturally distributed over several
                                    different sites, and cannot always be centralized to perform the DM tasks for cost reasons
                                    or because of practical and legal restrictions to data communication. Parallel data
                                    mining (PDM) and distributed data mining (DDM) are two closely related research fields
                                    aiming at the solution of scale and performance problems. We summarize the advantages
                                    they offer, looking at similarities and differences between the two approaches.</p>
                                    <p>
                                        PDM essentially deals with parallel systems that are tightly coupled. Among the
                                        architectures in this class, we find shared memory multiprocessors (SMP), distributed
                                        memory architectures, clusters of SMP machines, or large clusters with high-speed
                                        interconnection networks. DDM, on the contrary, concentrates on loosely coupled
                                        systems such as clusters of workstations connected by a slow LAN, geographically
                                        distributed sites over a wide area network, or even computational grid resources. The
                                        common advantages that parallel and distributed DM offer come from the removal of
                                        sequential architecture bottlenecks. We get higher I/O bandwidth, larger memory, and</p
        ></section ><br>
         <section class="container4"><p>computational power than the limits of existing sequential systems, all these factors
                                                    leading to lower response times and improved scalability to larger data sets. The common
                                                    drawback is that algorithm and application design becomes more complex in order to
                                                    enjoy higher performance. We need to devise algorithms and techniques that distribute
                                                    the I/O and the computation in parallel, minimizing communication and data transfers to
                                                    avoid wasting resources. There is of course a part of the theory and of the techniques
                                                    that is common to the distributed and parallel fields</p>
                                                    <p>
                                                        In this view, PDM has its central target in the exploitation of massive and possibly
                                                        fine-grained parallelism, paying closer attention to work synchronization and load
                                                        balancing, and exploiting high-performance I/O subsystems where available. PDM
                                                        applications deal with large and hard problems, and they are typically designed for
                                                        intensive mining of centralized archives.</p>
                                                        <p>y contrast, DDM techniques use a coarser computation grain and loose hypotheses on interconnection networks. DDM techniques are often targeted at distributed
                                                            databases, where data transfers are minimized or replaced by moving results in the form
                                                            of intermediate or final knowledge models. A widespread approach is independent
                                                            learning integrated with summarization and meta-learning techniques. The two fields of
                                                            PDM and DDM are not rigidly separated, however. Often the distinction between finegrained, highly synchronized parallelism, and coarse-grained parallelism gets blurred,
                                                            depending on problem characteristics, because massively parallel architectures and
                                                            large, loosely coupled clusters of sequential machines can be seen as extremes of a range
                                                            of architectures that have progressively changing nature. Actually, high-performance
                                                            computer architectures become more and more parallel, and it is definitely realistic to
                                                            study geographically distributed DDM algorithms where the local task is performed by
                                                            a PDM algorithm on a parallel machine.</p>
                                                            <h2>Integration of Parallel Tools into Data Mining
                                                                Environments</h2><br>
                                                                <p>
                                                                    It is now recognized that a crucial issue in the effectiveness of DM tools is the degree
                                                                    of interoperability with conventional databases, data warehouses and OLAP services.
                                                                    Maniatty and Zaki (2000) state several requirements for parallel DM systems, and the
                                                                    issues related to the integration are clearly underlined. They call System Transparency
                                                                    the ability to easily exploit file-system access as well as databases and data warehouses.
                                                                    This feature is not only a requirement of tool interoperability, but also an option to exploit
                                                                    the best software support available in different situations. Most mining algorithms,
                                                                    especially the parallel ones, are designed for flat-file mining. While this simplification
                                                                    eases initial code development, it imposes an overhead when working with higher-level
                                                                    data management supports (e.g., data dumping to flat files and view materialization from
                                                                    DBMS). Industry standards are being developed to address this issue in the sequential
                                                                    setting, and research is ongoing about the parallel case (see for instance the book by
                                                                    Freitas and Lavington, 1998). We can distinguish three main approaches:</p>
                                                                    <p>• Development of DM algorithms based on existing standard interfaces (e.g., SQL).
                                                                        Many algorithms have been rewritten or designed to work by means of DBMS
                                                                        primitives. DBSCAN is designed assuming the use of a spatial database system.</p>
            </section ><br>
          <section class="container5 ">  <p>• Development of a few new database-mining primitives within the frame of standard
                                                                        DBMS languages (a crucial issue is the expressive power that we may gain or lose).
                                                                        • Design of dedicated, possibly parallel mining interfaces to allow tight integration
                                                                        of the mining process with the data management.</p><br>
                                                                        <p>Pushing more of the computational effort into the data management support means
                                                                            exploiting the internal parallelism of modern database servers. On the other hand,
                                                                            scalability of such servers to massive parallelism is still a matter of research. While
                                                                            integration solutions are now emerging for sequential DM, this is not yet the case for
                                                                            parallel algorithms.</p>
                                                             <p> The bandwidth of I/O subsystems in parallel architectures is theoretically much
                                                                    higher than that of sequential ones, but a conventional file system or DBMS interface
                                                                    cannot easily exploit it. We need to use new software supports that are still far from being
                                                                    standards, and sometimes are architecture-specific. Parallel file systems, high-performance interfaces to parallel database servers are important resources to exploit for PDM.
                                                                    DDM must also take into account remote data servers, data transport layers, computational grid resources, and all the issues about security, availability, and fault tolerance
                                                                    that are commonplace for large distributed systems. Our approach is to develop a parallel
                                                                    programming environment that addresses the problem of parallelism exploitation within
                                                                    algorithms, while offering uniform interfacing characteristics with respect to different
                                                                    software and hardware resources for data management. Structured parallelism will be
                                                                    used to express the algorithmic parallelism, while an object-like interface will allow access
                                                                    to a number of useful services in a portable way, including other applications and
                                                                    CORBA-operated software.</p>
       <h2>STRUCTURED PARALLEL PROGRAMMING</h2>
       <p>
        Parallel programming exploits multiple computational resources in a coordinated
        effort to solve large and hard problems. In all but the really trivial cases, the classical
        problems of algorithm decomposition, load distribution, load balancing, and communication minimization have to be solved. Dealing directly with the degree of complexity
        given by communication management, concurrent behavior and architecture characteristics lead to programs that are error prone, difficult to debug and understand, and usually
        need complex performance tuning when ported to different architectures. The restricted
        approach to parallel programming takes into account these software engineering issues
        (Skillicorn & Talia, 1998). Restricted languages impose expressive constraints to the
        parallelism allowed in programs, which has to be defined using a given formalism.
        Requiring the explicit description of the parallel structure of programs leads to enhanced
        programmability, to higher semantics clearness, and to easier correctness verification.
        Compilation tools can take advantage of the explicit structure, resulting in more efficient
        compilation and automatic optimization of code. Porting sequential applications to
        parallel can proceed by progressive characterization of independent blocks of operations
        in the algorithm. They are moved to separate modules or code sections, which become
        the basic components of a high-level description. Prototype development and refinement
        is thus much faster than it is with low-level programming languages.</p>
    </section ></section>



        </center>
</body>
</html>